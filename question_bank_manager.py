import requests
from bs4 import BeautifulSoup
import json
import os
import time
import sys
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==========================================
# è¨­å®šèˆ‡å¸¸æ•¸
# ==========================================
DB_FILE = "questions.json"
PENDING_FILE = "pending_urls.txt"
INDEX_URL = "https://roddayeye.pixnet.net/blog/posts/15325785090"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
MAX_WORKERS = 10  # åŒæ™‚ 10 å€‹ç·šç¨‹

class QuestionBankManager:
    def __init__(self):
        self.questions_db = []
        self.known_urls = set()
        self.pending_urls = set()
        self.lock = threading.Lock()  # è³‡æ–™å¯«å…¥é–
        self.load_db()
        self.load_pending()

    def load_db(self):
        if os.path.exists(DB_FILE):
            try:
                with open(DB_FILE, 'r', encoding='utf-8') as f:
                    self.questions_db = json.load(f)
                    for q in self.questions_db:
                        if 'source_url' in q:
                            self.known_urls.add(q['source_url'])
                print(f"ğŸ“š å·²è¼‰å…¥ {len(self.questions_db)} ç­†é¡Œç›®è³‡æ–™")
            except Exception as e:
                print(f"âš ï¸ è®€å–è³‡æ–™åº«å¤±æ•—: {e}")
        else:
            print("â„¹ï¸ å°šæœªå»ºç«‹è³‡æ–™åº«")

    def save_db(self):
        try:
            # å­˜æª”æ™‚ä¹Ÿè¦é–ï¼Œé¿å…å¯«åˆ°ä¸€åŠè¢«è®€å–
            with self.lock:
                with open(DB_FILE, 'w', encoding='utf-8') as f:
                    json.dump(self.questions_db, f, ensure_ascii=False, indent=2)
            # print(f"ğŸ’¾ è³‡æ–™åº«å·²å„²å­˜ ({len(self.questions_db)} ç­†)")
        except Exception as e:
            print(f"âŒ å„²å­˜å¤±æ•—: {e}")

    def load_pending(self):
        if os.path.exists(PENDING_FILE):
            with open(PENDING_FILE, 'r', encoding='utf-8') as f:
                for line in f:
                    url = line.strip()
                    if url and url not in self.known_urls:
                        self.pending_urls.add(url)
            print(f"ğŸ“‹ å¾…è™•ç†ç¶²å€æ¸…å–®: {len(self.pending_urls)} ç­†")

    def save_pending(self):
        with self.lock:
            with open(PENDING_FILE, 'w', encoding='utf-8') as f:
                for url in self.pending_urls:
                    f.write(f"{url}\n")
    
    # ==========================
    # Logic
    # ==========================
    def get_index_titles(self):
        """
        æŠ“å–é¦–é ä¸¦å»ºç«‹ URL -> Title çš„å°ç…§è¡¨
        é€™æ¯”å¾å…§é æŠ“æ¨™é¡Œæº–ç¢º
        """
        print("ğŸ” [TitleMap] æ­£åœ¨åˆ†æé¦–é ä»¥å–å¾—æ­£ç¢ºæ¨™é¡Œ...")
        title_map = {}
        try:
            resp = requests.get(INDEX_URL, headers=HEADERS, timeout=30)
            soup = BeautifulSoup(resp.text, 'html.parser')
            links = soup.find_all('a', href=True)
            for a in links:
                href = a['href']
                text = a.get_text().strip()
                if "roddayeye.pixnet.net/blog/post/" in href and "è§£ç­”" in text:
                    title_map[href] = text
            print(f"âœ… [TitleMap] å–å¾— {len(title_map)} ç­†æ¨™é¡Œ")
        except Exception as e:
            print(f"âš ï¸ [TitleMap] æŠ“å–å¤±æ•—: {e}")
        return title_map

    def crawl_index(self):
        print("\nğŸ” æ­£åœ¨æŠ“å– Pixnet é¦–é é€£çµ...")
        try:
            # ç›´æ¥ä½¿ç”¨ get_index_titles çš„é‚è¼¯
            title_map = self.get_index_titles()
            found_count = 0
            
            for href in title_map.keys():
                if href not in self.known_urls:
                    self.pending_urls.add(href)
                    found_count += 1
            
            self.save_pending()
            print(f"âœ… é¦–é åˆ†æå®Œæˆï¼Œæ–°å¢ {found_count} å€‹å¾…æŠ“ç¶²å€ã€‚")
            print(f"   (ç›®å‰ç¸½è¨ˆå¾…æŠ“: {len(self.pending_urls)})")
            
        except Exception as e:
            print(f"âŒ æŠ“å–é¦–é å¤±æ•—: {e}")

    def add_manual_url(self):
        print("\nâœï¸  è«‹è¼¸å…¥ç¶²å€ (è¼¸å…¥ç©ºè¡ŒçµæŸ):")
        cnt = 0
        while True:
            url = input("> ").strip()
            if not url: break
            if url.startswith("http") and url not in self.known_urls:
                self.pending_urls.add(url)
                cnt += 1
        if cnt > 0:
            self.save_pending()
            print(f"âœ… å·²æ–°å¢ {cnt} å€‹ç¶²å€")

    def scrape_all(self):
        if not self.pending_urls:
            print("âš ï¸ æ²’æœ‰å¾…è™•ç†çš„ç¶²å€ã€‚è«‹å…ˆåŸ·è¡Œ [1] æˆ– [3]ã€‚")
            return

        # å…ˆå»ºç«‹æ¨™é¡Œå°ç…§è¡¨ (ç¢ºä¿æ¨™é¡Œæ­£ç¢º)
        self.title_map = self.get_index_titles()
        
        total = len(self.pending_urls)
        print(f"\nğŸš€ [å¤šç·šç¨‹æ¨¡å¼] é–‹å§‹ä¸‹è¼‰ä¸¦è§£æ {total} å€‹é é¢ (Workers={MAX_WORKERS})...")
        
        processed_count = 0
        new_q_count = 0
        
        # ä½¿ç”¨ ThreadPoolExecutor ä¸¦ç™¼åŸ·è¡Œ
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_url = {executor.submit(self.parse_single_page, url): url for url in list(self.pending_urls)}
            
            try:
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    processed_count += 1
                    
                    try:
                        q_list = future.result()
                        if q_list:
                            # å¯«å…¥è³‡æ–™éœ€è¦ä¸Šé–
                            with self.lock:
                                self.questions_db.extend(q_list)
                                self.known_urls.add(url)
                                self.pending_urls.discard(url)
                                
                            new_q_count += len(q_list)
                            print(f"[{processed_count}/{total}] âœ… {len(q_list)} é¡Œ | {q_list[0]['category']}")
                        else:
                            print(f"[{processed_count}/{total}] âš ï¸ ç„¡é¡Œç›® | {url.split('/')[-1]}")
                            with self.lock:
                                self.pending_urls.discard(url)
                                
                    except Exception as e:
                        print(f"[{processed_count}/{total}] âŒ å¤±æ•—: {url} ({e})")
                    
                    if processed_count % 20 == 0:
                        self.save_db()
                        self.save_pending()
                        print(f"--- è‡ªå‹•å­˜æª”ä¸­ (é€²åº¦ {processed_count}/{total}) ---")
                        
            except KeyboardInterrupt:
                print("\nğŸ›‘ ä½¿ç”¨è€…ä¸­æ–·ï¼æ­£åœ¨ç­‰å¾…åŸ·è¡Œä¸­çš„ç·šç¨‹çµæŸ...")
                executor.shutdown(wait=False)
                raise
 
        self.save_db()
        self.save_pending()
        print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼å…±æ–°å¢ {new_q_count} é¡Œã€‚")

    def parse_single_page(self, url):
        try:
            resp = requests.get(url, headers=HEADERS, timeout=20)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # å„ªå…ˆå¾ title_map å–å¾—æ­£ç¢ºæ¨™é¡Œ (è‹¥æœ‰)
            title_text = "æœªå‘½åæ¸¬é©—"
            if hasattr(self, 'title_map') and url in self.title_map:
                title_text = self.title_map[url]
            else:
                # Fallback: å˜—è©¦å¾é é¢æŠ“å–
                t_selectors = ['h2.post-title', 'h1', '.title']
                for sel in t_selectors:
                    found = soup.select_one(sel)
                    if found:
                        title_text = found.get_text().strip()
                        break
            
            table = soup.select_one('.article-content table, .article-content-inner table')
            if not table: return []

            rows = table.find_all('tr')
            extracted_qs = []
            
            current_q = None
            
            for tr in rows:
                tds = tr.find_all('td')
                if len(tds) < 1: continue
                
                cell_text_list = [td.get_text(strip=True) for td in tds]
                marker = cell_text_list[0]
                content_cell = tds[-1]
                
                # Remove obfuscation
                for span in content_cell.find_all('span'):
                    style = span.get('style', '')
                    if '255, 255, 255' in style or '255,255,255' in style or '#fff' in style.lower():
                        span.decompose()

                content = content_cell.get_text(strip=True)
                
                # Filter junk
                if 'roddayeye' in content or not content: continue
                
                if marker == 'Q':
                    if current_q: extracted_qs.append(current_q)
                    current_q = {
                        "category": title_text,
                        "source_url": url,
                        "question": content,
                        "options": [],
                        "answer": None
                    }
                elif current_q:
                    is_correct = (marker == 'v')
                    current_q['options'].append({
                        "text": content,
                        "correct": is_correct
                    })
                    if is_correct:
                        current_q['answer'] = content
            
            if current_q: extracted_qs.append(current_q)
            return extracted_qs

        except Exception as e:
            # print(f"parse error: {e}") 
            return []

    def main_loop(self):
        while True:
            print("\n=================================")
            print("   Pixnet é¡Œåº«æŠ“å–å·¥å…· (CLIç‰ˆ)")
            print("=================================")
            print("1. æŠ“å–é¦–é å…¨éƒ¨é€£çµ (Crawl Index)")
            print("2. é–‹å§‹ä¸‹è¼‰ä¸¦è§£æé¡Œç›® (Scrape All Pending)")
            print("3. æ‰‹å‹•è¼¸å…¥ç¶²å€ (Add Manual URL)")
            print("4. åŒ¯å‡º/é¡¯ç¤ºçµ±è¨ˆ (Stats)")
            print("q. é›¢é–‹ (Quit)")
            
            choice = input("\nè«‹é¸æ“‡åŠŸèƒ½ [1-4, q]: ").strip().lower()
            
            if choice == '1':
                self.crawl_index()
            elif choice == '2':
                self.scrape_all()
            elif choice == '3':
                self.add_manual_url()
            elif choice == '4':
                print(f"ç›®å‰è³‡æ–™åº«å…± {len(self.questions_db)} é¡Œ")
                print(f"å¾…æŠ“å–ç¶²å€: {len(self.pending_urls)} å€‹")
            elif choice == 'q':
                break
            else:
                print("ç„¡æ•ˆè¼¸å…¥")

if __name__ == "__main__":
    app = QuestionBankManager()
    app.main_loop()
