import requests
from bs4 import BeautifulSoup
import json
import os
import time
import sys
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==========================================
# è¨­å®šèˆ‡å¸¸æ•¸
# ==========================================
DB_FILE = "questions.json"
PENDING_FILE = "pending_urls.txt"
INDEX_URL = "https://roddayeye.pixnet.net/blog/posts/15325785090"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
MAX_WORKERS = 10  # åŒæ™‚ 10 å€‹ç·šç¨‹

class QuestionBankManager:
    def __init__(self):
        self.questions_db = []
        self.known_urls = set()
        self.pending_urls = set()
        self.lock = threading.Lock()  # è³‡æ–™å¯«å…¥é–
        self.load_db()
        self.load_pending()

    def load_db(self):
        if os.path.exists(DB_FILE):
            try:
                with open(DB_FILE, 'r', encoding='utf-8') as f:
                    self.questions_db = json.load(f)
                    for q in self.questions_db:
                        if 'source_url' in q:
                            self.known_urls.add(q['source_url'])
                print(f"ğŸ“š å·²è¼‰å…¥ {len(self.questions_db)} ç­†é¡Œç›®è³‡æ–™")
            except Exception as e:
                print(f"âš ï¸ è®€å–è³‡æ–™åº«å¤±æ•—: {e}")
        else:
            print("â„¹ï¸ å°šæœªå»ºç«‹è³‡æ–™åº«")

    def save_db(self):
        try:
            # å­˜æª”æ™‚ä¹Ÿè¦é–ï¼Œé¿å…å¯«åˆ°ä¸€åŠè¢«è®€å–
            with self.lock:
                with open(DB_FILE, 'w', encoding='utf-8') as f:
                    json.dump(self.questions_db, f, ensure_ascii=False, indent=2)
            # print(f"ğŸ’¾ è³‡æ–™åº«å·²å„²å­˜ ({len(self.questions_db)} ç­†)")
        except Exception as e:
            print(f"âŒ å„²å­˜å¤±æ•—: {e}")

    def load_pending(self):
        if os.path.exists(PENDING_FILE):
            with open(PENDING_FILE, 'r', encoding='utf-8') as f:
                for line in f:
                    url = line.strip()
                    if url and url not in self.known_urls:
                        self.pending_urls.add(url)
            print(f"ğŸ“‹ å¾…è™•ç†ç¶²å€æ¸…å–®: {len(self.pending_urls)} ç­†")

    def save_pending(self):
        with self.lock:
            with open(PENDING_FILE, 'w', encoding='utf-8') as f:
                for url in self.pending_urls:
                    f.write(f"{url}\n")
    
    # ==========================
    # Logic
    # ==========================
    def get_index_titles(self):
        """
        æŠ“å–é¦–é ä¸¦å»ºç«‹ URL -> Title çš„å°ç…§è¡¨
        é€™æ¯”å¾å…§é æŠ“æ¨™é¡Œæº–ç¢º
        """
        print("ğŸ” [TitleMap] æ­£åœ¨åˆ†æé¦–é ä»¥å–å¾—æ­£ç¢ºæ¨™é¡Œ...")
        title_map = {}
        try:
            resp = requests.get(INDEX_URL, headers=HEADERS, timeout=30)
            soup = BeautifulSoup(resp.text, 'html.parser')
            links = soup.find_all('a', href=True)
            for a in links:
                href = a['href']
                text = a.get_text().strip()
                if "roddayeye.pixnet.net/blog/post/" in href and "è§£ç­”" in text:
                    title_map[href] = text
            print(f"âœ… [TitleMap] å–å¾— {len(title_map)} ç­†æ¨™é¡Œ")
        except Exception as e:
            print(f"âš ï¸ [TitleMap] æŠ“å–å¤±æ•—: {e}")
        return title_map

    def auto_update_workflow(self):
        """
        [ä¸€éµæ›´æ–°] æ•´åˆæµç¨‹ï¼š
        1. æŠ“å–é¦–é é€£çµ & æ¨™é¡Œ
        2. æ¯”å°è³‡æ–™åº«ï¼Œæ‰¾å‡ºæ–°ç¶²å€
        3. ä¸‹è¼‰ä¸¦è§£ææ–°é¡Œç›®
        4. è‡ªå‹•ä¸Šå‚³åˆ° GitHub
        """
        print("\nğŸš€ [ä¸€éµæ›´æ–°] é–‹å§‹è‡ªå‹•åŒ–æµç¨‹...")
        
        # 1. å–å¾—æœ€æ–°æ¨™é¡Œèˆ‡é€£çµ
        title_map = self.get_index_titles()
        self.title_map = title_map
        
        # 2. æ‰¾å‡ºå°šæœªæ”¶éŒ„çš„ç¶²å€ (å»é‡æ ¸å¿ƒé‚è¼¯)
        new_urls = []
        for href in title_map.keys():
            if href not in self.known_urls:
                new_urls.append(href)
                self.pending_urls.add(href)
        
        print(f"ğŸ“Š åˆ†æçµæœ: {len(title_map)} ç¸½é€£çµ, {len(new_urls)} å€‹æ–°é€£çµå¾…æŠ“å–")
        
        if not new_urls and not self.pending_urls:
            print("âœ¨ ç›®å‰è³‡æ–™åº«å·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€æ›´æ–°ã€‚")
            return

        # 3. åŸ·è¡Œä¸‹è¼‰ (Scrape All)
        self.scrape_all(auto_push=True)

    def add_manual_url(self):
        """æ‰‹å‹•è¼¸å…¥ç¶²å€ (Debug ç”¨)"""
        print("\nâœï¸  è«‹è¼¸å…¥ç¶²å€ (è¼¸å…¥ç©ºè¡ŒçµæŸ):")
        cnt = 0
        while True:
            url = input("> ").strip()
            if not url: 
                break
            if url.startswith("http") and url not in self.known_urls:
                self.pending_urls.add(url)
                cnt += 1
            elif url in self.known_urls:
                print(f"   âš ï¸ æ­¤ç¶²å€å·²å­˜åœ¨è³‡æ–™åº«ä¸­")
        if cnt > 0:
            self.save_pending()
            print(f"âœ… å·²æ–°å¢ {cnt} å€‹ç¶²å€")
        else:
            print("â„¹ï¸ æ²’æœ‰æ–°å¢ä»»ä½•ç¶²å€")

    def scrape_all(self, auto_push=False):
        if not self.pending_urls:
            print("âš ï¸ æ²’æœ‰å¾…è™•ç†çš„ç¶²å€ã€‚")
            return

        # è‹¥æ²’å…ˆè·‘é auto_update_workflowï¼Œé€™é‚Šä¹Ÿå˜—è©¦æ‹¿ä¸€ä¸‹ title_map
        if not hasattr(self, 'title_map'):
             self.title_map = self.get_index_titles()

        total = len(self.pending_urls)
        print(f"\nğŸš€ [ä¸‹è¼‰] é–‹å§‹ä¸‹è¼‰ä¸¦è§£æ {total} å€‹é é¢ (Workers={MAX_WORKERS})...")
        
        processed_count = 0
        new_q_count = 0
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_url = {executor.submit(self.parse_single_page, url): url for url in list(self.pending_urls)}
            
            try:
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    processed_count += 1
                    
                    try:
                        q_list = future.result()
                        if q_list:
                            with self.lock:
                                # äºŒæ¬¡å»é‡æª¢æŸ¥ï¼šé›–ç„¶ url ä¸åœ¨ known_urlsï¼Œä½†ä»¥é˜²è¬ä¸€
                                if url not in self.known_urls:
                                    self.questions_db.extend(q_list)
                                    self.known_urls.add(url)
                                
                                self.pending_urls.discard(url)
                                
                            new_q_count += len(q_list)
                            print(f"[{processed_count}/{total}] âœ… {len(q_list)} é¡Œ | {q_list[0]['category']}")
                        else:
                            print(f"[{processed_count}/{total}] âš ï¸ ç„¡é¡Œç›® | {url.split('/')[-1]}")
                            with self.lock:
                                self.pending_urls.discard(url)
                                
                    except Exception as e:
                        print(f"[{processed_count}/{total}] âŒ å¤±æ•—: {url} ({e})")
                    
                    if processed_count % 20 == 0:
                        self.save_db()
                        self.save_pending()
                        print(f"--- è‡ªå‹•å­˜æª”ä¸­ (é€²åº¦ {processed_count}/{total}) ---")
                        
            except KeyboardInterrupt:
                print("\nğŸ›‘ ä½¿ç”¨è€…ä¸­æ–·ï¼æ­£åœ¨ç­‰å¾…åŸ·è¡Œä¸­çš„ç·šç¨‹çµæŸ...")
                executor.shutdown(wait=False)
                raise
 
        self.save_db()
        self.save_pending()
        print(f"\nğŸ‰ ä¸‹è¼‰å®Œæˆï¼å…±æ–°å¢ {new_q_count} é¡Œã€‚")
        
        if auto_push and new_q_count > 0:
            self.push_to_github(new_q_count)
        elif auto_push:
            print("â„¹ï¸ ç„¡æ–°é¡Œç›®ï¼Œç•¥é GitHub ä¸Šå‚³ã€‚")

    def push_to_github(self, count):
        print("\nâ˜ï¸ [Git] æ­£åœ¨ä¸Šå‚³è‡³ GitHub...")
        import subprocess
        try:
            subprocess.check_call(["git", "add", "questions.json"])
            subprocess.check_call(["git", "commit", "-m", f"Auto Update: Added {count} new questions"])
            # ä½¿ç”¨ -u åƒæ•¸ç¢ºä¿è¨­å®šä¸Šæ¸¸åˆ†æ”¯ (é¦–æ¬¡ push å¿…é ˆ)
            subprocess.check_call(["git", "push", "-u", "origin", "main"])
            print("âœ… GitHub æ›´æ–°æˆåŠŸï¼")
        except subprocess.CalledProcessError as e:
            if e.returncode == 1:
                # å¯èƒ½æ˜¯æ²’æœ‰ä»»ä½•è®Šæ›´éœ€è¦ commit
                print("â„¹ï¸ æ²’æœ‰è®Šæ›´éœ€è¦ä¸Šå‚³ã€‚")
            else:
                print(f"âŒ ä¸Šå‚³å¤±æ•—: {e}")
                print("   è«‹æª¢æŸ¥ç¶²è·¯æˆ– Git è¨­å®šã€‚")
        except Exception as e:
            print(f"âŒ ä¸Šå‚³å¤±æ•—: {e}")

    def parse_single_page(self, url):
        try:
            resp = requests.get(url, headers=HEADERS, timeout=20)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            title_text = "æœªå‘½åæ¸¬é©—"
            if hasattr(self, 'title_map') and url in self.title_map:
                title_text = self.title_map[url]
            else:
                t_selectors = ['h2.post-title', 'h1', '.title']
                for sel in t_selectors:
                    found = soup.select_one(sel)
                    if found:
                        title_text = found.get_text().strip()
                        break
            
            table = soup.select_one('.article-content table, .article-content-inner table')
            if not table: return []

            rows = table.find_all('tr')
            extracted_qs = []
            
            current_q = None
            
            for tr in rows:
                tds = tr.find_all('td')
                if len(tds) < 1: continue
                
                cell_text_list = [td.get_text(strip=True) for td in tds]
                marker = cell_text_list[0]
                content_cell = tds[-1]
                
                # Remove obfuscation
                for span in content_cell.find_all('span'):
                    style = span.get('style', '')
                    if '255, 255, 255' in style or '255,255,255' in style or '#fff' in style.lower():
                        span.decompose()

                content = content_cell.get_text(strip=True)
                
                # Filter junk
                if not content: continue
                
                # Clean Watermarks
                content = content.replace("r.o.d.d.a.y.e.y.e.", "").replace("roddayeye", "").strip()
                if not content: continue
                
                if marker == 'Q':
                    if current_q: extracted_qs.append(current_q)
                    current_q = {
                        "category": title_text,
                        "source_url": url,
                        "question": content,
                        "options": [],
                        "answer": None
                    }
                elif current_q:
                    is_correct = (marker == 'v')
                    current_q['options'].append({
                        "text": content,
                        "correct": is_correct
                    })
                    if is_correct:
                        current_q['answer'] = content
            
            if current_q: extracted_qs.append(current_q)
            return extracted_qs
    
        except Exception as e:
            return []

    def main_loop(self):
        while True:
            print("\n======================================")
            print("   Pixnet é¡Œåº«è‡ªå‹•åŒ–ç®¡ç†å™¨ (v2)")
            print("======================================")
            print("1. ğŸš€ ä¸€éµè‡ªå‹•æ›´æ–° (æŠ“å–+ä¸‹è¼‰+ä¸Šå‚³)")
            print("2. ğŸ“Š æŸ¥çœ‹ç›®å‰é¡Œåº«ç‹€æ…‹")
            print("3. ğŸ”§ æ‰‹å‹•è¼¸å…¥ç¶²å€ (Debugç”¨)")
            print("q. é›¢é–‹")
            
            choice = input("\nè«‹é¸æ“‡åŠŸèƒ½ [1, 2, 3, q]: ").strip().lower()
            
            if choice == '1':
                self.auto_update_workflow()
            elif choice == '2':
                print(f"ç›®å‰è³‡æ–™åº«å…± {len(self.questions_db)} é¡Œ")
                print(f"å·²çŸ¥ç¶²å€ (åŒ…å«å·²æŠ“å–): {len(self.known_urls)} å€‹")
                print(f" pending_urls (ä½‡åˆ—ä¸­): {len(self.pending_urls)} å€‹")
            elif choice == '3':
                self.add_manual_url()
                # æ‰‹å‹•åŠ å…¥å¾Œè©¢å•æ˜¯å¦ç«‹å³ä¸‹è¼‰
                if input("æ˜¯å¦ç«‹å³ä¸‹è¼‰? (y/n): ").lower() == 'y':
                    self.scrape_all(auto_push=False)
            elif choice == 'q':
                break
            else:
                print("ç„¡æ•ˆè¼¸å…¥")


if __name__ == "__main__":
    app = QuestionBankManager()
    app.main_loop()
